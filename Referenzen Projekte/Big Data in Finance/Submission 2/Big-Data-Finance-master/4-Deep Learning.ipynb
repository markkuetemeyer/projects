{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 2: Deep learning and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document aims to explain how the data was used to draw conclusions, which techniques and models were used to analyze and solve our problem. Our main goal was twofold. We wanted to identify if there is bias in a dataset and how to handle it. The structure of this paper represents our thinking and working process. We began by building several models, then analyzing the fairness of the model under consideration of specific metrics and finally showing two possibilities of how to handle bias in data. Last but not least we describe what implications for businesses this work has. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the exploratory data analysis in submission 1, we believe four variables:\n",
    "1. ZIP\n",
    "2. rent\n",
    "3. job stability\n",
    "4. occupation)\n",
    "cause the bias. \n",
    "\n",
    "Thus, our hypothesis is: \n",
    "\n",
    "**does removing these four variables reduce bias?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:2: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit\n",
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:2: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from numba.core.decorators import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (471136, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minority</th>\n",
       "      <th>sex</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>rent</th>\n",
       "      <th>education</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_size</th>\n",
       "      <th>payment_timing</th>\n",
       "      <th>year</th>\n",
       "      <th>job_stability</th>\n",
       "      <th>occupation</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.230650</td>\n",
       "      <td>36.050927</td>\n",
       "      <td>205168.022244</td>\n",
       "      <td>7600.292199</td>\n",
       "      <td>3.302193</td>\n",
       "      <td>0</td>\n",
       "      <td>3.015554</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45.891343</td>\n",
       "      <td>59.525251</td>\n",
       "      <td>187530.409981</td>\n",
       "      <td>5534.271289</td>\n",
       "      <td>3.843058</td>\n",
       "      <td>0</td>\n",
       "      <td>5.938132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46.775489</td>\n",
       "      <td>67.338108</td>\n",
       "      <td>196912.006690</td>\n",
       "      <td>2009.903438</td>\n",
       "      <td>2.059034</td>\n",
       "      <td>0</td>\n",
       "      <td>2.190777</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41.784839</td>\n",
       "      <td>24.067401</td>\n",
       "      <td>132911.650615</td>\n",
       "      <td>3112.280893</td>\n",
       "      <td>3.936169</td>\n",
       "      <td>0</td>\n",
       "      <td>1.725860</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41.744838</td>\n",
       "      <td>47.496605</td>\n",
       "      <td>161162.551205</td>\n",
       "      <td>1372.077093</td>\n",
       "      <td>3.709910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.883104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minority  sex  ZIP  rent  education        age         income    loan_size  \\\n",
       "0         1    0    1     1  57.230650  36.050927  205168.022244  7600.292199   \n",
       "1         1    0    1     1  45.891343  59.525251  187530.409981  5534.271289   \n",
       "2         1    0    1     1  46.775489  67.338108  196912.006690  2009.903438   \n",
       "3         1    0    1     1  41.784839  24.067401  132911.650615  3112.280893   \n",
       "4         1    0    1     1  41.744838  47.496605  161162.551205  1372.077093   \n",
       "\n",
       "   payment_timing  year  job_stability  occupation  default  \n",
       "0        3.302193     0       3.015554           1        1  \n",
       "1        3.843058     0       5.938132           1        1  \n",
       "2        2.059034     0       2.190777           1        1  \n",
       "3        3.936169     0       1.725860           1        1  \n",
       "4        3.709910     0       0.883104           1        1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pre_processed train dataset\n",
    "train = pd.read_csv('train_preprocessed.csv', index_col=0)\n",
    "print('train shape: ' + str(train.shape))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape: (160000, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minority</th>\n",
       "      <th>sex</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>rent</th>\n",
       "      <th>education</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_size</th>\n",
       "      <th>payment_timing</th>\n",
       "      <th>job_stability</th>\n",
       "      <th>year</th>\n",
       "      <th>occupation</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51.265723</td>\n",
       "      <td>25.710781</td>\n",
       "      <td>166455.209729</td>\n",
       "      <td>8064.951996</td>\n",
       "      <td>3.874735</td>\n",
       "      <td>43.764963</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.882849</td>\n",
       "      <td>39.689510</td>\n",
       "      <td>216752.885725</td>\n",
       "      <td>7166.701945</td>\n",
       "      <td>3.809001</td>\n",
       "      <td>46.903977</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.504545</td>\n",
       "      <td>25.847324</td>\n",
       "      <td>183764.480788</td>\n",
       "      <td>3322.045258</td>\n",
       "      <td>3.497214</td>\n",
       "      <td>63.453467</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.074111</td>\n",
       "      <td>26.381109</td>\n",
       "      <td>154057.004978</td>\n",
       "      <td>15.223904</td>\n",
       "      <td>3.535370</td>\n",
       "      <td>56.243840</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.916960</td>\n",
       "      <td>18.779902</td>\n",
       "      <td>143463.038107</td>\n",
       "      <td>7860.534547</td>\n",
       "      <td>3.663330</td>\n",
       "      <td>49.884194</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minority  sex  ZIP  rent  education        age         income    loan_size  \\\n",
       "0         1    0    1     1  51.265723  25.710781  166455.209729  8064.951996   \n",
       "1         0    0    1     0  58.882849  39.689510  216752.885725  7166.701945   \n",
       "2         0    0    1     0  56.504545  25.847324  183764.480788  3322.045258   \n",
       "3         1    0    1     1  47.074111  26.381109  154057.004978    15.223904   \n",
       "4         1    0    1     1  48.916960  18.779902  143463.038107  7860.534547   \n",
       "\n",
       "   payment_timing  job_stability  year  occupation  default  \n",
       "0        3.874735      43.764963    30           1        1  \n",
       "1        3.809001      46.903977    30           0        0  \n",
       "2        3.497214      63.453467    30           0        0  \n",
       "3        3.535370      56.243840    30           1        0  \n",
       "4        3.663330      49.884194    30           1        0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pre_processed test dataset\n",
    "test = pd.read_csv('test_preprocessed.csv', index_col=0)\n",
    "print('test shape: ' + str(test.shape))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify train and test dataset\n",
    "\n",
    "**Terminology:**\n",
    "- Bias dataset = dataset that contains ZIP, rent, job_stability and occupation variables\n",
    "- Non-Bias dataset = dataset that does **NOT** contain ZIP, rent, jot_stability and occupation variables\n",
    "- default variable = is our target variables\n",
    "- minority variable = is our protected group (we're trying to identify bias caused by minority)\n",
    "\n",
    "**Note**\n",
    "- We drop sex variable since it is a protected group. Thus, we're not allowed to use sex in our model. Also, we're focusing on bias caused by minority and not sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "X_tr_nob = train.drop(['default', 'minority', 'rent', 'job_stability', 'occupation', 'sex', 'ZIP'], axis=1)\n",
    "X_tr_bias = train.drop(['default', 'minority', 'sex'], axis=1)\n",
    "y_train= train['default']\n",
    "\n",
    "#test dataset\n",
    "X_te_nob = test.drop(['default', 'minority', 'rent', 'job_stability', 'occupation', 'sex', 'ZIP'], axis=1)\n",
    "X_te_bias = test.drop(['default', 'minority', 'sex'], axis=1)\n",
    "y_test = test['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale X train/test Bias\n",
    "scaler = StandardScaler()\n",
    "X_fit_bias = scaler.fit(X_tr_bias)\n",
    "\n",
    "X_train_bias = X_fit_bias.transform(X_tr_bias)\n",
    "X_test_bias = X_fit_bias.transform(X_te_bias)\n",
    "\n",
    "#Scale X train/test Non-Bias\n",
    "X_fit_nob = scaler.fit(X_tr_nob)\n",
    "\n",
    "X_train_nob = X_fit_nob.transform(X_tr_nob)\n",
    "X_test_nob = X_fit_nob.transform(X_te_nob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear classification: Base Model\n",
    "\n",
    "Linear classification (LogisticRegression) will serve as our base model. We will compare results (accuracy) to deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with bias: 37.01%\n",
      "Accuracy with no bias: 41.49%\n"
     ]
    }
   ],
   "source": [
    "#bias\n",
    "lr_bias = LogisticRegression(solver='lbfgs', random_state=seed)\n",
    "lr_bias.fit(X_train_bias, y_train)\n",
    "\n",
    "y_pred_lr_bias = lr_bias.predict(X_test_bias)\n",
    "accuracy_lr_bias = accuracy_score(y_pred_lr_bias, y_test)\n",
    "\n",
    "\n",
    "#no bias\n",
    "lr_nob = LogisticRegression(solver='lbfgs', random_state=seed)\n",
    "lr_nob.fit(X_train_nob, y_train)\n",
    "\n",
    "y_pred_lr_nob = lr_nob.predict(X_test_nob)\n",
    "accuracy_lr_nob = accuracy_score(y_pred_lr_nob, y_test)\n",
    "\n",
    "#print accuracy\n",
    "print('Accuracy with bias: ' + str(round(accuracy_lr_bias*100,2)) + '%')\n",
    "print('Accuracy with no bias: ' + str(round(accuracy_lr_nob*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.33      0.47    136055\n",
      "           1       0.13      0.58      0.22     23945\n",
      "\n",
      "    accuracy                           0.37    160000\n",
      "   macro avg       0.48      0.46      0.34    160000\n",
      "weighted avg       0.72      0.37      0.44    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print classification report - BIAS\n",
    "print('Bias classification report')\n",
    "print(classification_report(y_test, y_pred_lr_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Bias classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.38      0.53    136055\n",
      "           1       0.14      0.59      0.23     23945\n",
      "\n",
      "    accuracy                           0.41    160000\n",
      "   macro avg       0.49      0.49      0.38    160000\n",
      "weighted avg       0.74      0.41      0.48    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print classification report - NO BIAS\n",
    "print('Non-Bias classification report')\n",
    "print(classification_report(y_test, y_pred_lr_nob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1], y=0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "7995    0\n",
      "7996    0\n",
      "7997    0\n",
      "7998    0\n",
      "7999    0\n",
      "Name: default, Length: 471136, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#stop if no improvement in loss after 3 epochs\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "#class weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df):\n",
    "    ## Initialize model.\n",
    "    model = Sequential() #if you want a recurrent NN, then specify here Recurrent()\n",
    "\n",
    "    ## 1st Layer\n",
    "    model.add(Dense(64, input_dim=df.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5)) #50% of neurons are de-activated randomly per batch. helps with generalisation.\n",
    "\n",
    "    ## 2nd Layer\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5)) #dropout number is arbitrary. trail and error. \n",
    "\n",
    "    ## Adding Softmax Layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid')) #softmax used for classification. softmax = [0,1]\n",
    "\n",
    "    ## Define loss function\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], weighted_metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "Inspired by the code on this website: \n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "def kfold_validation(X, y):\n",
    "\n",
    "    X_df = pd.DataFrame(X)\n",
    "    cvscores = []\n",
    "    \n",
    "    for train, test in kfold.split(X_df, y):\n",
    "        model = create_model(X_df)\n",
    "        model.fit(X_df.iloc[train], y.iloc[train], epochs=20, batch_size=1000, shuffle=True, verbose=0, \n",
    "                  callbacks=[es_callback], class_weight=class_weights)\n",
    "        scores = model.evaluate(X_df.iloc[test], y.iloc[test], verbose=0)\n",
    "        \n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 99.98%\n",
      "accuracy: 99.98%\n",
      "accuracy: 99.98%\n",
      "accuracy: 99.99%\n",
      "accuracy: 99.99%\n",
      "99.98% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "#cross-validaiton\n",
    "FNN_bias = kfold_validation(X_train_bias, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 471136 samples, validate on 160000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0793 - accuracy: 0.9705 - accuracy_1: 0.9705 - val_loss: 2.7812 - val_accuracy: 0.4946 - val_accuracy_1: 0.4946\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0041 - accuracy: 0.9995 - accuracy_1: 0.9995 - val_loss: 3.5917 - val_accuracy: 0.4848 - val_accuracy_1: 0.4848\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0030 - accuracy: 0.9997 - accuracy_1: 0.9997 - val_loss: 4.3423 - val_accuracy: 0.4545 - val_accuracy_1: 0.4545\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0026 - accuracy: 0.9997 - accuracy_1: 0.9997 - val_loss: 4.8941 - val_accuracy: 0.4527 - val_accuracy_1: 0.4527\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0023 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.0414 - val_accuracy: 0.4590 - val_accuracy_1: 0.4590\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0022 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.1433 - val_accuracy: 0.4708 - val_accuracy_1: 0.4708\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0021 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.5775 - val_accuracy: 0.4574 - val_accuracy_1: 0.4574\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0019 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.5585 - val_accuracy: 0.4725 - val_accuracy_1: 0.4725\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0019 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.0034 - val_accuracy: 0.5101 - val_accuracy_1: 0.5101\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0019 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.7613 - val_accuracy: 0.4790 - val_accuracy_1: 0.4790\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0018 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.0199 - val_accuracy: 0.4921 - val_accuracy_1: 0.4921\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0018 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.6083 - val_accuracy: 0.5107 - val_accuracy_1: 0.5107\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.0017 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.9325 - val_accuracy: 0.5039 - val_accuracy_1: 0.5039\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.0018 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.4388 - val_accuracy: 0.4836 - val_accuracy_1: 0.4836\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.0017 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.6371 - val_accuracy: 0.5338 - val_accuracy_1: 0.5338\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.0016 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 7.4826 - val_accuracy: 0.4324 - val_accuracy_1: 0.4324\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.0017 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.6216 - val_accuracy: 0.5024 - val_accuracy_1: 0.5024\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.0016 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.2679 - val_accuracy: 0.5706 - val_accuracy_1: 0.5706\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.0017 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.6322 - val_accuracy: 0.5602 - val_accuracy_1: 0.5602\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.0014 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.1915 - val_accuracy: 0.5498 - val_accuracy_1: 0.5498\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.0015 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.3900 - val_accuracy: 0.5469 - val_accuracy_1: 0.5469\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.0015 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.0883 - val_accuracy: 0.6405 - val_accuracy_1: 0.6405\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.0014 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.1778 - val_accuracy: 0.5723 - val_accuracy_1: 0.5723\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.0015 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 5.1127 - val_accuracy: 0.6514 - val_accuracy_1: 0.6514\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.0014 - accuracy: 0.9998 - accuracy_1: 0.9998 - val_loss: 6.7176 - val_accuracy: 0.5542 - val_accuracy_1: 0.5542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x135d2fe10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the kfold model is giving wierd results. \n",
    "#thus, switching to this model\n",
    "#I think issue stems from stratifiedKfold not working, i.e. I'm doing something wrong\n",
    "FNN_bias = create_model(X_train_bias)\n",
    "FNN_bias.fit(X_train_bias, y_train, epochs=100, batch_size=1000, shuffle=True, verbose=2, callbacks=[es_callback],\n",
    "           class_weight=class_weights, validation_data=(X_test_bias, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.57      0.69    136055\n",
      "           1       0.16      0.45      0.23     23945\n",
      "\n",
      "    accuracy                           0.55    160000\n",
      "   macro avg       0.51      0.51      0.46    160000\n",
      "weighted avg       0.75      0.55      0.62    160000\n",
      "\n",
      "\n",
      "Bias confusion matrix\n",
      "[[77931 58124]\n",
      " [13208 10737]]\n"
     ]
    }
   ],
   "source": [
    "#predict default with test dataset\n",
    "y_pred_bias = FNN_bias.predict_classes(X_test_bias)\n",
    "\n",
    "print('Bias classification report')\n",
    "print(classification_report(y_test, y_pred_bias))\n",
    "print()\n",
    "\n",
    "print('Bias confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with NO-BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.02%\n",
      "accuracy: 49.98%\n",
      "accuracy: 49.98%\n",
      "accuracy: 50.00%\n",
      "accuracy: 50.00%\n",
      "50.00% (+/- 0.01%)\n"
     ]
    }
   ],
   "source": [
    "#cross-validation\n",
    "FNN_nob = kfold_validation(X_train_nob, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 471136 samples, validate on 160000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.6966 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6980 - val_accuracy: 0.1722 - val_accuracy_1: 0.1722\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.6933 - accuracy: 0.5004 - accuracy_1: 0.5004 - val_loss: 0.6944 - val_accuracy: 0.1726 - val_accuracy_1: 0.1726\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4979 - accuracy_1: 0.4979 - val_loss: 0.6940 - val_accuracy: 0.1709 - val_accuracy_1: 0.1709\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5012 - accuracy_1: 0.5012 - val_loss: 0.6961 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4994 - accuracy_1: 0.4994 - val_loss: 0.6926 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6963 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5004 - accuracy_1: 0.5004 - val_loss: 0.6943 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4997 - accuracy_1: 0.4997 - val_loss: 0.6971 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6933 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4993 - accuracy_1: 0.4993 - val_loss: 0.6908 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4995 - accuracy_1: 0.4995 - val_loss: 0.6900 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6938 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6929 - val_accuracy: 0.8497 - val_accuracy_1: 0.8497\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5003 - accuracy_1: 0.5003 - val_loss: 0.6915 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.6931 - accuracy: 0.5007 - accuracy_1: 0.5007 - val_loss: 0.6919 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.4986 - accuracy_1: 0.4986 - val_loss: 0.6919 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.6932 - accuracy: 0.5008 - accuracy_1: 0.5008 - val_loss: 0.6919 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5002 - accuracy_1: 0.5002 - val_loss: 0.6937 - val_accuracy: 0.1499 - val_accuracy_1: 0.1499\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6930 - val_accuracy: 0.8502 - val_accuracy_1: 0.8502\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.6932 - accuracy: 0.5007 - accuracy_1: 0.5007 - val_loss: 0.6938 - val_accuracy: 0.1499 - val_accuracy_1: 0.1499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x135a89f98>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the kfold model is giving wierd results. \n",
    "#thus, switching to this model\n",
    "#I think issue stems from stratifiedKfold not working, i.e. I'm doing something wrong\n",
    "FNN_nob_ = create_model(X_train_nob)\n",
    "FNN_nob_.fit(X_train_nob, y_train,  epochs=100, shuffle=True, batch_size=1000, verbose=2, callbacks=[es_callback],\n",
    "           class_weight=class_weights, validation_data=(X_test_nob, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 85.03%\n",
      "accuracy: 14.97%\n",
      "accuracy: 14.97%\n",
      "accuracy: 85.03%\n",
      "accuracy: 81.65%\n",
      "accuracy: 85.03%\n",
      "accuracy: 14.97%\n",
      "accuracy: 14.97%\n",
      "accuracy: 14.97%\n",
      "accuracy: 14.97%\n",
      "42.65% (+/- 33.92%)\n"
     ]
    }
   ],
   "source": [
    "#cross_validation on test\n",
    "#train the model 10 times and predict. \n",
    "#take the average.\n",
    "cvscores = []\n",
    "for x in range(10):\n",
    "    \n",
    "    FNN_nob_.fit(X_train_nob, y_train,  epochs=100, shuffle=True, batch_size=1000, verbose=0, \n",
    "                 callbacks=[es_callback], class_weight=class_weights)\n",
    "    \n",
    "    scores = FNN_nob_.evaluate(X_test_nob, y_test, verbose=0)\n",
    "        \n",
    "    print(\"%s: %.2f%%\" % (FNN_nob_.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "        \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Bias classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    136055\n",
      "           1       0.15      1.00      0.26     23945\n",
      "\n",
      "    accuracy                           0.15    160000\n",
      "   macro avg       0.07      0.50      0.13    160000\n",
      "weighted avg       0.02      0.15      0.04    160000\n",
      "\n",
      "\n",
      "Non-Bias confusion matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0 136055]\n",
      " [     0  23945]]\n"
     ]
    }
   ],
   "source": [
    "#predict default with test dataset\n",
    "y_pred_nob = FNN_nob_.predict_classes(X_test_nob)\n",
    "\n",
    "print('Non-Bias classification report')\n",
    "print(classification_report(y_test, y_pred_nob))\n",
    "print()\n",
    "\n",
    "print('Non-Bias confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred_nob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fairness of model \n",
    "\n",
    "We will use three metrics to identify bias:\n",
    "1. Demographic Parity \n",
    "    - Positive rate\n",
    "2. Equal Opportunity \n",
    "    - True positive rate\n",
    "3. Equalised Odds \n",
    "    - Combination of true positive rate and true negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split train and test dataset into 4 groups\n",
    "# Minority + Default\n",
    "# Minority + No-default\n",
    "# Non-Minority + Default\n",
    "# Non-Minority + No-default\n",
    "\n",
    "#these groups are needed to calculate fairness metrics\n",
    "\n",
    "\n",
    "def bias_analysis(X_test_df, y_test_df, df_column_names, model, bias, minority):\n",
    "    \"\"\" \n",
    "    X_test_df = X_test after StandardScaler transformation\n",
    "    y_test_df = y_test \n",
    "    df_columns_names = X_test dataframe before StandardScaler transformation\n",
    "    model = deep learning model\n",
    "    bias = 0 or 1\n",
    "    minority = 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    #join minority variable to X and y dataframes to differentiate between minority and non-minority\n",
    "    X_test_min = np.concatenate((X_test_df, \n",
    "                                 test['minority'].values.reshape(-1,1)), \n",
    "                                axis=1)\n",
    "    y_test_min = np.concatenate((y_test_df.values.reshape(-1,1),\n",
    "                                 test['minority'].values.reshape(-1,1)), \n",
    "                                axis=1)\n",
    "    \n",
    "    #column headings for dataframe that will be converted from array\n",
    "    columns = np.insert(df_column_names.columns, X_test_df.shape[1], 'minority')\n",
    "    \n",
    "    #convert numpy array to dataframe\n",
    "    df_X_test = pd.DataFrame(X_test_min, columns=columns)\n",
    "    df_y_test = pd.DataFrame(y_test_min, columns=['default','minority'])\n",
    "    \n",
    "    #subset minority==1, minority group\n",
    "    X_minority = df_X_test[df_X_test['minority']==minority].values\n",
    "    y_minority = df_y_test[df_y_test['minority']==minority].values\n",
    "    \n",
    "    #predict y\n",
    "    y_pred_minority = model.predict_classes(X_minority[:,:X_test_df.shape[1]])\n",
    "\n",
    "    #evaluate accuracy\n",
    "    loss_dl_min, accuracy_dl_min, accuracy_weighted_dl_min = model.evaluate(X_minority[:,:X_test_df.shape[1]], \n",
    "                                                                            y_minority[:,0])\n",
    "    \n",
    "    confusion = confusion_matrix(y_minority[:,0], y_pred_minority)\n",
    "    \n",
    "    if bias == 1:\n",
    "        if minority == 1:\n",
    "        \n",
    "            print('Test loss bias minority: ' + str(round(loss_dl_min, 2)))\n",
    "            print('Test accuracy bias minority: ' + str(round(accuracy_dl_min*100, 2)) + '%')\n",
    "\n",
    "            print()\n",
    "\n",
    "            return confusion\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Test loss bias non-minority: ' + str(round(loss_dl_min, 2)))\n",
    "            print('Test accuracy bias non-minority: ' + str(round(accuracy_dl_min*100, 2)) + '%')\n",
    "\n",
    "            print()\n",
    "\n",
    "            return confusion\n",
    "      \n",
    "    #bias=0\n",
    "    if bias == 0:\n",
    "        if minority == 1:\n",
    "\n",
    "            print('Test loss non-bias minority: ' + str(round(loss_dl_min, 2)))\n",
    "            print('Test accuracy non-bias minority: ' + str(round(accuracy_dl_min*100, 2)) + '%')\n",
    "\n",
    "            print()\n",
    "\n",
    "            return confusion\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print('Test loss non-bias non-minority: ' + str(round(loss_dl_min, 2)))\n",
    "            print('Test accuracy non-bias non-minority: ' + str(round(accuracy_dl_min*100, 2)) + '%')\n",
    "\n",
    "            print()\n",
    "\n",
    "            return confusion\n",
    "        \n",
    "    if bias==2:\n",
    "                    \n",
    "            print('Test loss downsampled non-minority: ' + str(round(loss_dl_min, 2)))\n",
    "            print('Test accuracy downsampled non-minority: ' + str(round(accuracy_dl_min*100, 2)) + '%')\n",
    "\n",
    "            print()\n",
    "\n",
    "            return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to reduce confusion regarding which variales to use in bias_analysis function\n",
    "\n",
    "def bias_check(bias, minority):\n",
    "    \"\"\"\n",
    "    bias = 0 or 1\n",
    "        bias==1 means dataset is biased\n",
    "    minority = 0 or 1\n",
    "        minority==1 means minority is analysed\n",
    "        \n",
    "    bias ==2 \n",
    "        when train dataset has been downsampled\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if bias == 1:\n",
    "        result = bias_analysis(X_test_bias, y_test, X_te_bias, FNN_bias, bias=bias, minority=minority)\n",
    "        return result\n",
    "    \n",
    "    if bias == 0:\n",
    "        result = bias_analysis(X_test_nob, y_test, X_te_nob, FNN_nob, bias=bias, minority=minority)\n",
    "        return result\n",
    "    \n",
    "    if bias == 2:\n",
    "        result = bias_analysis(X_test_bias, y_test, X_te_bias, FNN_ds, bias=bias, minority=minority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confusion martrix\n",
    "\n",
    "Confusion matrix is needed to calculate fp, tn, fn, tp, which are needed to calculate fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80348/80348 [==============================] - 2s 19us/step\n",
      "Test loss bias minority: 10.13\n",
      "Test accuracy bias minority: 27.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for BIAS and Minority\n",
    "confusion_bias_minority = bias_check(bias=1, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79652/79652 [==============================] - 2s 23us/step\n",
      "Test loss bias non-minority: 3.27\n",
      "Test accuracy bias non-minority: 84.05%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for BIAS and Non-Minority\n",
    "confusion_bias_non_minority = bias_check(bias=1, minority=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80348/80348 [==============================] - 2s 25us/step\n",
      "Test loss non-bias minority: 0.69\n",
      "Test accuracy non-bias minority: 84.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for Non-BIAS and Minority\n",
    "confusion_non_bias_minority = bias_check(bias=0, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79652/79652 [==============================] - 2s 30us/step\n",
      "Test loss non-bias non-minority: 0.69\n",
      "Test accuracy non-bias non-minority: 85.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for Non-Bias and Non-Minority\n",
    "confusion_non_bias_non_minority = bias_check(bias=0, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic parity is achieved when the positive rate in minority and non-minority are similar. Positive in our case means default. We assume a 5% difference between minority and non-minority to mean that they are similar. \n",
    "\n",
    "According to Demographic Parity:\n",
    "- **Bias** dataset is not fair.\n",
    "\n",
    "- **Non-Bias** dataset is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really good article explaining fairness measures in ML\n",
    "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/demographic_parity.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate demographic parity\n",
    "\n",
    "def demographic_parity(result, bias=0, minority=0):\n",
    "    \"\"\"\n",
    "    bias = 0 or 1\n",
    "        bias==1 means dataset is biased\n",
    "    minority = 0 or 1\n",
    "        minority==1 means minority is analysed\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tn, fp, fn, tp = result.ravel()\n",
    "    #tn, fp, fn, tp = bias_check(bias, minority).ravel()\n",
    "    \n",
    "    temp = (fp + tp) / (tn + fp + fn + tp)\n",
    "    demo_parity = round(temp*100, 2) \n",
    "    \n",
    "    if bias == 1:\n",
    "        if minority == 0:\n",
    "            print('Demographic parity for bias non-minority: ' + str(demo_parity) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print('Demographic parity for bias minority: ' + str(demo_parity) + '%')\n",
    "   \n",
    "    if bias == 0:\n",
    "        if minority == 0:\n",
    "            print('Demographic parity for non-bias non-minority: ' + str(demo_parity) + '%')    \n",
    "            \n",
    "        else: \n",
    "            print('Demographic parity for non-bias minority: ' + str(demo_parity) + '%')  \n",
    "            \n",
    "    if bias == 2:\n",
    "        if minority == 0:\n",
    "            print('Demographic parity for downsampled non-minority: ' + str(demo_parity) + '%')    \n",
    "            \n",
    "        else: \n",
    "            print('Demographic parity for downsampled minority: ' + str(demo_parity) + '%')  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity for bias minority: 83.76%\n",
      "Demographic parity for bias non-minority: 1.96%\n"
     ]
    }
   ],
   "source": [
    "#BIAS\n",
    "demographic_parity(confusion_bias_minority, bias=1, minority=1)\n",
    "demographic_parity(confusion_bias_non_minority, bias=1, minority=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity for non-bias minority: 0.0%\n",
      "Demographic parity for non-bias non-minority: 0.0%\n"
     ]
    }
   ],
   "source": [
    "#Non-BIAS\n",
    "demographic_parity(confusion_non_bias_minority, bias=0, minority=1)\n",
    "demographic_parity(confusion_non_bias_non_minority, bias=0, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal Opportunity is achieved when the true positive rate in minority and non-minority are similar. Positive in our case means default. We assume a 5% difference between minority and non-minority to mean that they are similar. \n",
    "\n",
    "According to Equal Opportunity:\n",
    "- **Bias** dataset is not fair.\n",
    "\n",
    "- **Non-Bias** dataset is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/equal_opportunity.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate equal opportunity\n",
    "def equal_opportunity(result, bias=0, minority=0):\n",
    "    \"\"\"\n",
    "    bias = 0 or 1\n",
    "        bias==1 means dataset is biased\n",
    "    minority = 0 or 1\n",
    "        minority==1 means minority is analysed\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tn, fp, fn, tp = result.ravel()\n",
    "    #tn, fp, fn, tp = bias_check(bias, minority).ravel()\n",
    "    \n",
    "    temp = tp / (fn + tp)\n",
    "    equal_opportunity = round(temp*100, 2) \n",
    "    \n",
    "    if bias == 1:\n",
    "        if minority == 0:\n",
    "            print('Equal opportunity for bias non-minority: ' + str(equal_opportunity) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print('Equal opportunity for bias minority: ' + str(equal_opportunity) + '%')\n",
    "\n",
    "    \n",
    "    if bias == 0:\n",
    "        if minority == 0:\n",
    "            print('Equal opportunity for non-bias non-minority: ' + str(equal_opportunity) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print('Equal opportunity for non-bias minority: ' + str(equal_opportunity) + '%')\n",
    "\n",
    "        \n",
    "    if bias==2:\n",
    "        if minority == 0:\n",
    "            print('Equal opportunity for downsampled non-minority: ' + str(equal_opportunity) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print('Equal opportunity for downsampled minority: ' + str(equal_opportunity) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal opportunity for bias minority: 85.91%\n",
      "Equal opportunity for bias non-minority: 3.05%\n"
     ]
    }
   ],
   "source": [
    "#BIAS\n",
    "equal_opportunity(confusion_bias_minority, bias=1, minority=1)\n",
    "equal_opportunity(confusion_bias_non_minority, bias=1, minority=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal opportunity for non-bias minority: 0.0%\n",
      "Equal opportunity for non-bias non-minority: 0.0%\n"
     ]
    }
   ],
   "source": [
    "#Non-BIAS\n",
    "equal_opportunity(confusion_non_bias_minority, bias=0, minority=1)\n",
    "equal_opportunity(confusion_non_bias_non_minority, bias=0, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equalised Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equalised Odds is achieved when the true positive rate and the true negative rate in minority and non-minority are similar. Positive in our case means default. We assume a 5% difference between minority and non-minority to mean that they are similar. \n",
    "\n",
    "According to Equalised Odds:\n",
    "- **Bias** dataset is not fair.\n",
    "\n",
    "- **Non-Bias** dataset is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/equalised_odds.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate equlaised odds\n",
    "def equalised_odds(result, bias=0, minority=0):\n",
    "    \"\"\"\n",
    "    bias = 0 or 1\n",
    "        bias==1 means dataset is biased\n",
    "    minority = 0 or 1\n",
    "        minority==1 means minority is analysed\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tn, fp, fn, tp = result.ravel()\n",
    "    #tn, fp, fn, tp = bias_check(bias, minority).ravel()\n",
    "    \n",
    "    temp = tp / (fn + tp)\n",
    "    true_positive_rate = round(temp*100, 2)\n",
    "    \n",
    "    temp = tn / (fp + tn)\n",
    "    true_negative_rate = round(temp*100, 2)\n",
    "    \n",
    "    if bias == 1:\n",
    "        if minority == 0:\n",
    "            print()\n",
    "            print('True positive rate for bias non-minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for bias non-minority: ' + str(true_negative_rate) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print()\n",
    "            print('True positive rate for bias minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for bias minority: ' + str(true_negative_rate) + '%')\n",
    "\n",
    "    \n",
    "    if bias == 0:\n",
    "        if minority == 0:\n",
    "            print()\n",
    "            print('True positive rate for non-bias non-minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for non-bias non-minority: ' + str(true_negative_rate) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print()\n",
    "            print('True positive rate for non-bias minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for non-bias minority: ' + str(true_negative_rate) + '%')\n",
    "\n",
    "        \n",
    "    if bias==2:\n",
    "        if minority == 0:\n",
    "            print()\n",
    "            print('True positive rate for downsampled non-minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for downsampled non-minority: ' + str(true_negative_rate) + '%')\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print()\n",
    "            print('True positive rate for downsampled minority: ' + str(true_positive_rate) + '%')\n",
    "            print('True negative rate for downsampled minority: ' + str(true_negative_rate) + '%')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True positive rate for bias minority: 85.91%\n",
      "True negative rate for bias minority: 16.62%\n",
      "\n",
      "True positive rate for bias non-minority: 3.05%\n",
      "True negative rate for bias non-minority: 98.23%\n",
      "\n",
      "True positive rate for non-bias minority: 0.0%\n",
      "True negative rate for non-bias minority: 100.0%\n",
      "\n",
      "True positive rate for non-bias non-minority: 0.0%\n",
      "True negative rate for non-bias non-minority: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#BIAS\n",
    "equalised_odds(confusion_bias_minority, bias=1, minority=1)\n",
    "equalised_odds(confusion_bias_non_minority, bias=1, minority=0)\n",
    "\n",
    "#Non-BIAS\n",
    "equalised_odds(confusion_non_bias_minority, bias=0, minority=1)\n",
    "equalised_odds(confusion_non_bias_non_minority, bias=0, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Discussion\n",
    "\n",
    "Overall, the bias dataset is biased (not fair) since none of the above metrics (demographic parity, equal opportunity, or equalised odds) are within 5% difference. \n",
    "\n",
    "The non-bias dataset is deemed to be fair according to the above metrics (demographic parity, equal opportunity, and equalised odds) since all results are within 5% difference. This proves our hypothesis that removing the biased variables (ZIP, rent, job_stability and occupation) does remove bias, i.e. makes the model fairer. \n",
    "\n",
    "However, removing four variables might reduce accuracy (lose signal) and not be in a business's interest. Therefore, we will try another approach, down-sampling, to reduce bias and keep the four variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Down-sampled Train Dataset\n",
    "\n",
    "With down-sampling, we aim to retain the four variables (ZIP, rent, job_stability and occupation), which are the cause of bias in our train dataset, and at the same time reduce bias.\n",
    "\n",
    "We hope to reduce bias by down-sampling the train dataset to have 50% positive rate (default rate) in minority and non-minority group. To achieve this we will down-sample the following to 250 cases (we use 250 cases sine it is the small common denominator among the four groups):\n",
    "- Minority and default = 250 cases\n",
    "- Minority and non-default = 250 cases\n",
    "- Non-Minority and default = 250 cases\n",
    "- Non-Minority and default = 250 cases\n",
    "\n",
    "Overall, we will have a down-sampled train dataset with 1'000 cases (rows). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this website for downsampling:\n",
    "https://elitedatascience.com/imbalanced-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    500\n",
      "0    500\n",
      "Name: minority, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minority  default\n",
       "0         0          250\n",
       "          1          250\n",
       "1         0          250\n",
       "          1          250\n",
       "Name: minority, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downsampling\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_non_minority_0 = train[(train['minority']==0) & (train['default']==0)]\n",
    "df_non_minority_1 = train[(train['minority']==0) & (train['default']==1)]\n",
    "\n",
    "df_minority_0 = train[(train['minority']==1) & (train['default']==0)]\n",
    "df_minority_1 = train[(train['minority']==1) & (train['default']==1)]\n",
    " \n",
    "\n",
    "# Downsample non_minority, default == 0\n",
    "df_non_minority_0_downsampled = resample(df_non_minority_0, \n",
    "                                         replace=False,    # sample without replacement\n",
    "                                         n_samples=250,     # to match minority class\n",
    "                                         random_state=seed) # reproducible results\n",
    "\n",
    "# Downsample non_minority, default == 1\n",
    "df_non_minority_1_downsampled = resample(df_non_minority_1, \n",
    "                                         replace=False,    # sample without replacement\n",
    "                                         n_samples=250,     # to match minority class\n",
    "                                         random_state=seed) # reproducible results\n",
    "\n",
    "# Downsample minority, default == 1\n",
    "df_minority_1_downsampled = resample(df_minority_1, \n",
    "                                         replace=False,    # sample without replacement\n",
    "                                         n_samples=250,     # to match minority class\n",
    "                                         random_state=seed) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_non_minority_0_downsampled,\n",
    "                            df_non_minority_1_downsampled,\n",
    "                            df_minority_0,\n",
    "                            df_minority_1_downsampled\n",
    "                           ])\n",
    " \n",
    "# Display new class counts\n",
    "print(df_downsampled['minority'].value_counts())\n",
    "\n",
    "#Display distribution for minority, default\n",
    "df_downsampled.groupby([\"minority\", \"default\"])[\"minority\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with independent variables (X) and indepedent variable (y)\n",
    "X_ds = df_downsampled.drop(['default', 'minority', 'sex'], axis=1)\n",
    "y_downsampled = df_downsampled['default']\n",
    "\n",
    "#Scale X train/test bias\n",
    "scaler = StandardScaler()\n",
    "X_fit_bias_ds = scaler.fit(X_ds)\n",
    "\n",
    "X_downsampled = X_fit_bias_ds.transform(X_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression - Down-sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy with downsampled dataset: 36.82%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.32      0.46    136055\n",
      "           1       0.14      0.65      0.24     23945\n",
      "\n",
      "    accuracy                           0.37    160000\n",
      "   macro avg       0.49      0.48      0.35    160000\n",
      "weighted avg       0.73      0.37      0.43    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "lr_ds = LogisticRegression(solver='lbfgs')\n",
    "lr_ds.fit(X_downsampled, y_downsampled)\n",
    "\n",
    "y_pred_ds = lr_ds.predict(X_test_bias)\n",
    "accuracy_lr_ds = accuracy_score(y_pred_ds, y_test)\n",
    "\n",
    "\n",
    "#print accuracy\n",
    "print('LogisticRegression accuracy with downsampled dataset: ' + str(round(accuracy_lr_ds*100,2)) + '%')\n",
    "print()\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(y_test, y_pred_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning - Down-sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 87.50%\n",
      "accuracy: 89.00%\n",
      "accuracy: 89.00%\n",
      "accuracy: 87.00%\n",
      "accuracy: 86.50%\n",
      "87.80% (+/- 1.03%)\n"
     ]
    }
   ],
   "source": [
    "#train deep learning model with 5-kfold validation\n",
    "FNN_ds = kfold_validation(X_downsampled, y_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 160000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.7180 - accuracy: 0.5380 - accuracy_1: 0.5380 - val_loss: 0.9340 - val_accuracy: 0.1676 - val_accuracy_1: 0.1676\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.7147 - accuracy: 0.5350 - accuracy_1: 0.5350 - val_loss: 0.9200 - val_accuracy: 0.1688 - val_accuracy_1: 0.1688\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.7175 - accuracy: 0.5160 - accuracy_1: 0.5160 - val_loss: 0.9075 - val_accuracy: 0.1697 - val_accuracy_1: 0.1697\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6973 - accuracy: 0.5450 - accuracy_1: 0.5450 - val_loss: 0.8950 - val_accuracy: 0.1715 - val_accuracy_1: 0.1715\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6858 - accuracy: 0.5860 - accuracy_1: 0.5860 - val_loss: 0.8831 - val_accuracy: 0.1734 - val_accuracy_1: 0.1734\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6685 - accuracy: 0.6000 - accuracy_1: 0.6000 - val_loss: 0.8720 - val_accuracy: 0.1759 - val_accuracy_1: 0.1759\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6708 - accuracy: 0.5770 - accuracy_1: 0.5770 - val_loss: 0.8614 - val_accuracy: 0.1782 - val_accuracy_1: 0.1782\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6649 - accuracy: 0.5850 - accuracy_1: 0.5850 - val_loss: 0.8515 - val_accuracy: 0.1816 - val_accuracy_1: 0.1816\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6582 - accuracy: 0.5930 - accuracy_1: 0.5930 - val_loss: 0.8427 - val_accuracy: 0.1853 - val_accuracy_1: 0.1853\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6299 - accuracy: 0.6440 - accuracy_1: 0.6440 - val_loss: 0.8356 - val_accuracy: 0.1892 - val_accuracy_1: 0.1892\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6291 - accuracy: 0.6400 - accuracy_1: 0.6400 - val_loss: 0.8287 - val_accuracy: 0.1940 - val_accuracy_1: 0.1940\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.6144 - accuracy: 0.6570 - accuracy_1: 0.6570 - val_loss: 0.8223 - val_accuracy: 0.2000 - val_accuracy_1: 0.2000\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6305 - accuracy: 0.6320 - accuracy_1: 0.6320 - val_loss: 0.8159 - val_accuracy: 0.2071 - val_accuracy_1: 0.2071\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6140 - accuracy: 0.6600 - accuracy_1: 0.6600 - val_loss: 0.8101 - val_accuracy: 0.2148 - val_accuracy_1: 0.2148\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.6052 - accuracy: 0.6700 - accuracy_1: 0.6700 - val_loss: 0.8049 - val_accuracy: 0.2228 - val_accuracy_1: 0.2228\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5901 - accuracy: 0.6940 - accuracy_1: 0.6940 - val_loss: 0.8003 - val_accuracy: 0.2313 - val_accuracy_1: 0.2313\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5858 - accuracy: 0.7190 - accuracy_1: 0.7190 - val_loss: 0.7959 - val_accuracy: 0.2405 - val_accuracy_1: 0.2405\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5912 - accuracy: 0.6950 - accuracy_1: 0.6950 - val_loss: 0.7923 - val_accuracy: 0.2498 - val_accuracy_1: 0.2498\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5661 - accuracy: 0.7160 - accuracy_1: 0.7160 - val_loss: 0.7894 - val_accuracy: 0.2593 - val_accuracy_1: 0.2593\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5722 - accuracy: 0.7340 - accuracy_1: 0.7340 - val_loss: 0.7858 - val_accuracy: 0.2701 - val_accuracy_1: 0.2701\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5699 - accuracy: 0.7210 - accuracy_1: 0.7210 - val_loss: 0.7827 - val_accuracy: 0.2816 - val_accuracy_1: 0.2816\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5608 - accuracy: 0.7160 - accuracy_1: 0.7160 - val_loss: 0.7798 - val_accuracy: 0.2925 - val_accuracy_1: 0.2925\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5353 - accuracy: 0.7600 - accuracy_1: 0.7600 - val_loss: 0.7771 - val_accuracy: 0.3041 - val_accuracy_1: 0.3041\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5412 - accuracy: 0.7430 - accuracy_1: 0.7430 - val_loss: 0.7748 - val_accuracy: 0.3149 - val_accuracy_1: 0.3149\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5352 - accuracy: 0.7570 - accuracy_1: 0.7570 - val_loss: 0.7729 - val_accuracy: 0.3255 - val_accuracy_1: 0.3255\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5256 - accuracy: 0.7570 - accuracy_1: 0.7570 - val_loss: 0.7710 - val_accuracy: 0.3355 - val_accuracy_1: 0.3355\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5130 - accuracy: 0.7670 - accuracy_1: 0.7670 - val_loss: 0.7702 - val_accuracy: 0.3440 - val_accuracy_1: 0.3440\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5232 - accuracy: 0.7610 - accuracy_1: 0.7610 - val_loss: 0.7690 - val_accuracy: 0.3527 - val_accuracy_1: 0.3527\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5110 - accuracy: 0.7720 - accuracy_1: 0.7720 - val_loss: 0.7677 - val_accuracy: 0.3611 - val_accuracy_1: 0.3611\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5015 - accuracy: 0.7970 - accuracy_1: 0.7970 - val_loss: 0.7662 - val_accuracy: 0.3709 - val_accuracy_1: 0.3709\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.4936 - accuracy: 0.7910 - accuracy_1: 0.7910 - val_loss: 0.7647 - val_accuracy: 0.3802 - val_accuracy_1: 0.3802\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.4823 - accuracy: 0.8070 - accuracy_1: 0.8070 - val_loss: 0.7636 - val_accuracy: 0.3887 - val_accuracy_1: 0.3887\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.4740 - accuracy: 0.8190 - accuracy_1: 0.8190 - val_loss: 0.7626 - val_accuracy: 0.3970 - val_accuracy_1: 0.3970\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.4818 - accuracy: 0.8120 - accuracy_1: 0.8120 - val_loss: 0.7607 - val_accuracy: 0.4070 - val_accuracy_1: 0.4070\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.4620 - accuracy: 0.8150 - accuracy_1: 0.8150 - val_loss: 0.7591 - val_accuracy: 0.4174 - val_accuracy_1: 0.4174\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.4520 - accuracy: 0.8340 - accuracy_1: 0.8340 - val_loss: 0.7571 - val_accuracy: 0.4279 - val_accuracy_1: 0.4279\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.4404 - accuracy: 0.8350 - accuracy_1: 0.8350 - val_loss: 0.7555 - val_accuracy: 0.4380 - val_accuracy_1: 0.4380\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.4465 - accuracy: 0.8210 - accuracy_1: 0.8210 - val_loss: 0.7543 - val_accuracy: 0.4472 - val_accuracy_1: 0.4472\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4487 - accuracy: 0.8240 - accuracy_1: 0.8240 - val_loss: 0.7524 - val_accuracy: 0.4570 - val_accuracy_1: 0.4570\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4357 - accuracy: 0.8390 - accuracy_1: 0.8390 - val_loss: 0.7511 - val_accuracy: 0.4660 - val_accuracy_1: 0.4660\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4331 - accuracy: 0.8320 - accuracy_1: 0.8320 - val_loss: 0.7504 - val_accuracy: 0.4738 - val_accuracy_1: 0.4738\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4105 - accuracy: 0.8530 - accuracy_1: 0.8530 - val_loss: 0.7501 - val_accuracy: 0.4809 - val_accuracy_1: 0.4809\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4163 - accuracy: 0.8590 - accuracy_1: 0.8590 - val_loss: 0.7504 - val_accuracy: 0.4870 - val_accuracy_1: 0.4870\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4247 - accuracy: 0.8270 - accuracy_1: 0.8270 - val_loss: 0.7508 - val_accuracy: 0.4922 - val_accuracy_1: 0.4922\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4080 - accuracy: 0.8430 - accuracy_1: 0.8430 - val_loss: 0.7519 - val_accuracy: 0.4964 - val_accuracy_1: 0.4964\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3966 - accuracy: 0.8580 - accuracy_1: 0.8580 - val_loss: 0.7537 - val_accuracy: 0.4995 - val_accuracy_1: 0.4995\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.3797 - accuracy: 0.8540 - accuracy_1: 0.8540 - val_loss: 0.7566 - val_accuracy: 0.5006 - val_accuracy_1: 0.5006\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4007 - accuracy: 0.8560 - accuracy_1: 0.8560 - val_loss: 0.7597 - val_accuracy: 0.5016 - val_accuracy_1: 0.5016\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3781 - accuracy: 0.8630 - accuracy_1: 0.8630 - val_loss: 0.7639 - val_accuracy: 0.5013 - val_accuracy_1: 0.5013\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3809 - accuracy: 0.8560 - accuracy_1: 0.8560 - val_loss: 0.7692 - val_accuracy: 0.4999 - val_accuracy_1: 0.4999\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3652 - accuracy: 0.8770 - accuracy_1: 0.8770 - val_loss: 0.7742 - val_accuracy: 0.4990 - val_accuracy_1: 0.4990\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3804 - accuracy: 0.8560 - accuracy_1: 0.8560 - val_loss: 0.7797 - val_accuracy: 0.4977 - val_accuracy_1: 0.4977\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3588 - accuracy: 0.8740 - accuracy_1: 0.8740 - val_loss: 0.7855 - val_accuracy: 0.4963 - val_accuracy_1: 0.4963\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3608 - accuracy: 0.8720 - accuracy_1: 0.8720 - val_loss: 0.7914 - val_accuracy: 0.4954 - val_accuracy_1: 0.4954\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3324 - accuracy: 0.8740 - accuracy_1: 0.8740 - val_loss: 0.7987 - val_accuracy: 0.4933 - val_accuracy_1: 0.4933\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3469 - accuracy: 0.8610 - accuracy_1: 0.8610 - val_loss: 0.8066 - val_accuracy: 0.4907 - val_accuracy_1: 0.4907\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3373 - accuracy: 0.8790 - accuracy_1: 0.8790 - val_loss: 0.8147 - val_accuracy: 0.4883 - val_accuracy_1: 0.4883\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3267 - accuracy: 0.8860 - accuracy_1: 0.8860 - val_loss: 0.8227 - val_accuracy: 0.4860 - val_accuracy_1: 0.4860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      " - 0s - loss: 0.3316 - accuracy: 0.8900 - accuracy_1: 0.8900 - val_loss: 0.8299 - val_accuracy: 0.4850 - val_accuracy_1: 0.4850\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3340 - accuracy: 0.8680 - accuracy_1: 0.8680 - val_loss: 0.8365 - val_accuracy: 0.4846 - val_accuracy_1: 0.4846\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.3288 - accuracy: 0.8910 - accuracy_1: 0.8910 - val_loss: 0.8444 - val_accuracy: 0.4832 - val_accuracy_1: 0.4832\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.3185 - accuracy: 0.8750 - accuracy_1: 0.8750 - val_loss: 0.8521 - val_accuracy: 0.4821 - val_accuracy_1: 0.4821\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.3159 - accuracy: 0.8840 - accuracy_1: 0.8840 - val_loss: 0.8612 - val_accuracy: 0.4798 - val_accuracy_1: 0.4798\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3088 - accuracy: 0.8810 - accuracy_1: 0.8810 - val_loss: 0.8690 - val_accuracy: 0.4786 - val_accuracy_1: 0.4786\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.3098 - accuracy: 0.8720 - accuracy_1: 0.8720 - val_loss: 0.8765 - val_accuracy: 0.4781 - val_accuracy_1: 0.4781\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.3131 - accuracy: 0.8810 - accuracy_1: 0.8810 - val_loss: 0.8846 - val_accuracy: 0.4770 - val_accuracy_1: 0.4770\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3081 - accuracy: 0.8800 - accuracy_1: 0.8800 - val_loss: 0.8914 - val_accuracy: 0.4770 - val_accuracy_1: 0.4770\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3005 - accuracy: 0.8870 - accuracy_1: 0.8870 - val_loss: 0.8968 - val_accuracy: 0.4783 - val_accuracy_1: 0.4783\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2877 - accuracy: 0.8870 - accuracy_1: 0.8870 - val_loss: 0.9021 - val_accuracy: 0.4793 - val_accuracy_1: 0.4793\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2901 - accuracy: 0.8930 - accuracy_1: 0.8930 - val_loss: 0.9050 - val_accuracy: 0.4826 - val_accuracy_1: 0.4826\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2884 - accuracy: 0.8770 - accuracy_1: 0.8770 - val_loss: 0.9071 - val_accuracy: 0.4862 - val_accuracy_1: 0.4862\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2924 - accuracy: 0.8930 - accuracy_1: 0.8930 - val_loss: 0.9106 - val_accuracy: 0.4886 - val_accuracy_1: 0.4886\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2736 - accuracy: 0.8970 - accuracy_1: 0.8970 - val_loss: 0.9139 - val_accuracy: 0.4907 - val_accuracy_1: 0.4907\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2729 - accuracy: 0.8890 - accuracy_1: 0.8890 - val_loss: 0.9195 - val_accuracy: 0.4913 - val_accuracy_1: 0.4913\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2776 - accuracy: 0.8870 - accuracy_1: 0.8870 - val_loss: 0.9260 - val_accuracy: 0.4912 - val_accuracy_1: 0.4912\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2595 - accuracy: 0.9050 - accuracy_1: 0.9050 - val_loss: 0.9323 - val_accuracy: 0.4913 - val_accuracy_1: 0.4913\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2748 - accuracy: 0.8830 - accuracy_1: 0.8830 - val_loss: 0.9381 - val_accuracy: 0.4916 - val_accuracy_1: 0.4916\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2573 - accuracy: 0.8950 - accuracy_1: 0.8950 - val_loss: 0.9449 - val_accuracy: 0.4909 - val_accuracy_1: 0.4909\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2610 - accuracy: 0.9070 - accuracy_1: 0.9070 - val_loss: 0.9498 - val_accuracy: 0.4920 - val_accuracy_1: 0.4920\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2512 - accuracy: 0.8960 - accuracy_1: 0.8960 - val_loss: 0.9536 - val_accuracy: 0.4941 - val_accuracy_1: 0.4941\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2538 - accuracy: 0.9030 - accuracy_1: 0.9030 - val_loss: 0.9558 - val_accuracy: 0.4970 - val_accuracy_1: 0.4970\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2547 - accuracy: 0.8850 - accuracy_1: 0.8850 - val_loss: 0.9601 - val_accuracy: 0.4983 - val_accuracy_1: 0.4983\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2545 - accuracy: 0.8870 - accuracy_1: 0.8870 - val_loss: 0.9639 - val_accuracy: 0.4998 - val_accuracy_1: 0.4998\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2505 - accuracy: 0.9030 - accuracy_1: 0.9030 - val_loss: 0.9678 - val_accuracy: 0.5011 - val_accuracy_1: 0.5011\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2411 - accuracy: 0.8990 - accuracy_1: 0.8990 - val_loss: 0.9719 - val_accuracy: 0.5023 - val_accuracy_1: 0.5023\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2423 - accuracy: 0.8990 - accuracy_1: 0.8990 - val_loss: 0.9744 - val_accuracy: 0.5043 - val_accuracy_1: 0.5043\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2267 - accuracy: 0.9050 - accuracy_1: 0.9050 - val_loss: 0.9768 - val_accuracy: 0.5065 - val_accuracy_1: 0.5065\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2325 - accuracy: 0.9060 - accuracy_1: 0.9060 - val_loss: 0.9796 - val_accuracy: 0.5084 - val_accuracy_1: 0.5084\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2438 - accuracy: 0.8980 - accuracy_1: 0.8980 - val_loss: 0.9809 - val_accuracy: 0.5114 - val_accuracy_1: 0.5114\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2265 - accuracy: 0.9090 - accuracy_1: 0.9090 - val_loss: 0.9811 - val_accuracy: 0.5148 - val_accuracy_1: 0.5148\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2271 - accuracy: 0.9050 - accuracy_1: 0.9050 - val_loss: 0.9824 - val_accuracy: 0.5177 - val_accuracy_1: 0.5177\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2314 - accuracy: 0.9070 - accuracy_1: 0.9070 - val_loss: 0.9856 - val_accuracy: 0.5188 - val_accuracy_1: 0.5188\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2257 - accuracy: 0.8940 - accuracy_1: 0.8940 - val_loss: 0.9876 - val_accuracy: 0.5211 - val_accuracy_1: 0.5211\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2297 - accuracy: 0.8970 - accuracy_1: 0.8970 - val_loss: 0.9899 - val_accuracy: 0.5231 - val_accuracy_1: 0.5231\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2243 - accuracy: 0.9120 - accuracy_1: 0.9120 - val_loss: 0.9925 - val_accuracy: 0.5244 - val_accuracy_1: 0.5244\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2360 - accuracy: 0.8980 - accuracy_1: 0.8980 - val_loss: 0.9962 - val_accuracy: 0.5249 - val_accuracy_1: 0.5249\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2231 - accuracy: 0.9000 - accuracy_1: 0.9000 - val_loss: 0.9998 - val_accuracy: 0.5256 - val_accuracy_1: 0.5256\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2187 - accuracy: 0.9010 - accuracy_1: 0.9010 - val_loss: 1.0038 - val_accuracy: 0.5257 - val_accuracy_1: 0.5257\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2187 - accuracy: 0.8970 - accuracy_1: 0.8970 - val_loss: 1.0065 - val_accuracy: 0.5265 - val_accuracy_1: 0.5265\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2190 - accuracy: 0.9060 - accuracy_1: 0.9060 - val_loss: 1.0087 - val_accuracy: 0.5275 - val_accuracy_1: 0.5275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13ee10f98>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the kfold model is giving wierd results. \n",
    "#thus, switching to this model\n",
    "#I think issue stems from stratifiedKfold not working, i.e. I'm doing something wrong\n",
    "FNN_ds = create_model(X_downsampled)\n",
    "FNN_ds.fit(X_downsampled, y_downsampled, epochs=100, batch_size=1000, shuffle=True, verbose=2, callbacks=[es_callback],\n",
    "           class_weight=class_weights, validation_data=(X_test_bias, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning accuracy with downsampled dataset: 52.75%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.49      0.64    136055\n",
      "           1       0.20      0.71      0.31     23945\n",
      "\n",
      "    accuracy                           0.53    160000\n",
      "   macro avg       0.55      0.60      0.48    160000\n",
      "weighted avg       0.80      0.53      0.59    160000\n",
      "\n",
      "\n",
      "Down-sampled confusion matrix\n",
      "[[     0 136055]\n",
      " [     0  23945]]\n"
     ]
    }
   ],
   "source": [
    "#accuracy of downsampled dataset on test dataset\n",
    "y_pred_dl_ds = FNN_ds.predict_classes(X_test_bias, verbose=0)\n",
    "\n",
    "accuracy_dl_ds = accuracy_score(y_pred_dl_ds, y_test)\n",
    "\n",
    "#print accuracy\n",
    "print('Deep Learning accuracy with downsampled dataset: ' + str(round(accuracy_dl_ds*100,2)) + '%')\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_pred_dl_ds))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Down-sampled confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred_nob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness - Identifying Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80348/80348 [==============================] - 1s 17us/step\n",
      "Test loss downsampled non-minority: 0.79\n",
      "Test accuracy downsampled non-minority: 62.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate confusion matrix\n",
    "confusion_ds_minority = bias_analysis(X_test_bias, y_test, X_te_bias, FNN_ds, bias=2, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79652/79652 [==============================] - 1s 17us/step\n",
      "Test loss downsampled non-minority: 1.23\n",
      "Test accuracy downsampled non-minority: 42.43%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_ds_non_minority = bias_analysis(X_test_bias, y_test, X_te_bias, FNN_ds, bias=2, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity - Downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity for downsampled minority: 41.27%\n"
     ]
    }
   ],
   "source": [
    "#demographic parity for downsampled minority\n",
    "demographic_parity(confusion_ds_minority, bias=2, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity for downsampled non-minority: 66.1%\n"
     ]
    }
   ],
   "source": [
    "#demographic parity for downsampled non-minority\n",
    "demographic_parity(confusion_ds_non_minority, bias=2, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Opportunity - Downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal opportunity for downsampled minority: 64.12%\n"
     ]
    }
   ],
   "source": [
    "#equal opportunity for downsampled minority. \n",
    "equal_opportunity(confusion_ds_minority, bias=2, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal opportunity for downsampled non-minority: 78.62%\n"
     ]
    }
   ],
   "source": [
    "#equal opportunity for downsampled non-minority. \n",
    "equal_opportunity(confusion_ds_non_minority, bias=2, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalised Odds - Downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True positive rate for downsampled minority: 64.12%\n",
      "True negative rate for downsampled minority: 62.78%\n"
     ]
    }
   ],
   "source": [
    "#equalised odds for downsampled minority\n",
    "equalised_odds(confusion_ds_minority, bias=2, minority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True positive rate for downsampled non-minority: 78.62%\n",
      "True negative rate for downsampled non-minority: 36.09%\n"
     ]
    }
   ],
   "source": [
    "#equalised odds for donwsampled non-minority\n",
    "equalised_odds(confusion_ds_non_minority, bias=2, minority=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Discussion - Down-Sampled\n",
    "\n",
    "According to demographic parity, equal opportunity and equalised odds, the down-sampled reduces bias compared to the bias dataset, however, bias still exists in the down-sampled dataset. None of the metrics have a differences between minority and non-minority of less than 5%. \n",
    "\n",
    "Down-sampling a dataset from +450'000 rows to only 1'000 is drastic and not ideal. Therefore, we will try to use disparate impact remover to remove the bias from the train dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Disparate Impact Remover\n",
    "\n",
    "Disparate impact remover on bias dataset did not work, since it only predicts 1s at an accuracy of 15%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook on how to remove bias in a variable: https://nbviewer.jupyter.org/github/srnghn/bias-mitigation-examples/blob/master/Bias%20Mitigation%20with%20Disparate%20Impact%20Remover.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset and prepare for disparate impact remover\n",
    "train_BLD = BinaryLabelDataset(favorable_label='1',\n",
    "                                unfavorable_label='0',\n",
    "                                df=train,\n",
    "                                label_names=['default'],\n",
    "                                protected_attribute_names=['minority'],\n",
    "                                unprivileged_protected_attributes=['1'])\n",
    "test_BLD = BinaryLabelDataset(favorable_label='1',\n",
    "                                unfavorable_label='0',\n",
    "                                df=test,\n",
    "                                label_names=['default'],\n",
    "                                protected_attribute_names=['minority'],\n",
    "                                unprivileged_protected_attributes=['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform dataset to remove disparate impact\n",
    "di = DisparateImpactRemover(repair_level=1.0)\n",
    "rp_train = di.fit_transform(train_BLD)\n",
    "rp_test = di.fit_transform(test_BLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 471136 samples, validate on 160000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 646.5594 - accuracy: 0.4995 - accuracy_1: 0.4995 - val_loss: 0.6987 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 2/100\n",
      " - 2s - loss: 4.1756 - accuracy: 0.5002 - accuracy_1: 0.5002 - val_loss: 0.6927 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 3/100\n",
      " - 2s - loss: 2.1113 - accuracy: 0.5000 - accuracy_1: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.5326 - accuracy: 0.4990 - accuracy_1: 0.4990 - val_loss: 0.6940 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 5/100\n",
      " - 2s - loss: 1.1074 - accuracy: 0.4997 - accuracy_1: 0.4997 - val_loss: 0.6938 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.8910 - accuracy: 0.5002 - accuracy_1: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.8523 - accuracy: 0.4990 - accuracy_1: 0.4990 - val_loss: 0.6929 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.8080 - accuracy: 0.4993 - accuracy_1: 0.4993 - val_loss: 0.6937 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.8007 - accuracy: 0.4992 - accuracy_1: 0.4992 - val_loss: 0.6965 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.8038 - accuracy: 0.4986 - accuracy_1: 0.4986 - val_loss: 0.6934 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.7406 - accuracy: 0.4987 - accuracy_1: 0.4987 - val_loss: 0.6927 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.7195 - accuracy: 0.4995 - accuracy_1: 0.4995 - val_loss: 0.6930 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.7302 - accuracy: 0.4991 - accuracy_1: 0.4991 - val_loss: 0.6919 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.7181 - accuracy: 0.4998 - accuracy_1: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.7173 - accuracy: 0.4996 - accuracy_1: 0.4996 - val_loss: 0.6921 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.7100 - accuracy: 0.4996 - accuracy_1: 0.4996 - val_loss: 0.6944 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.7214 - accuracy: 0.4989 - accuracy_1: 0.4989 - val_loss: 0.6905 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.7028 - accuracy: 0.5000 - accuracy_1: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.6993 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6938 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.7072 - accuracy: 0.5000 - accuracy_1: 0.5000 - val_loss: 0.6909 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.6964 - accuracy: 0.5009 - accuracy_1: 0.5009 - val_loss: 0.6898 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.6958 - accuracy: 0.5006 - accuracy_1: 0.5006 - val_loss: 0.6910 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.6958 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6926 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.6997 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6928 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.6943 - accuracy: 0.4999 - accuracy_1: 0.4999 - val_loss: 0.6919 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.6940 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6952 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.6949 - accuracy: 0.5000 - accuracy_1: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.6962 - accuracy: 0.5006 - accuracy_1: 0.5006 - val_loss: 0.6925 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.6997 - accuracy: 0.4987 - accuracy_1: 0.4987 - val_loss: 0.6936 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.6938 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6924 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 31/100\n",
      " - 2s - loss: 0.6937 - accuracy: 0.4998 - accuracy_1: 0.4998 - val_loss: 0.6952 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.7014 - accuracy: 0.4987 - accuracy_1: 0.4987 - val_loss: 0.6934 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.6942 - accuracy: 0.4996 - accuracy_1: 0.4996 - val_loss: 0.6934 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.6932 - accuracy: 0.5000 - accuracy_1: 0.5000 - val_loss: 0.6962 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.6932 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6918 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.6934 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6930 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 37/100\n",
      " - 2s - loss: 0.6969 - accuracy: 0.5001 - accuracy_1: 0.5001 - val_loss: 0.6955 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.6947 - accuracy: 0.4994 - accuracy_1: 0.4994 - val_loss: 0.6917 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.6932 - accuracy: 0.5005 - accuracy_1: 0.5005 - val_loss: 0.6953 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1353dfeb8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train deep learning model\n",
    "FNN_rp = create_model(rp_train.features[:, 2:]) #first two columns dropped since sex and minority\n",
    "FNN_rp.fit(rp_train.features[:, 2:], rp_train.labels, epochs=100, batch_size=1000, shuffle=True, verbose=2, \n",
    "           callbacks=[es_callback], validation_data=(rp_test.features[:, 2:], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict labels\n",
    "di_preds = FNN_rp.predict(rp_test.features[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minority</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minority     preds\n",
       "0       1.0  0.501546\n",
       "1       0.0  0.501546\n",
       "2       0.0  0.501546\n",
       "3       1.0  0.501546\n",
       "4       1.0  0.501546"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe with labels and minority\n",
    "di_pred_np = np.concatenate((test[['minority']],\n",
    "                                di_preds), \n",
    "                                axis=1)\n",
    "di_pred_df = pd.DataFrame(di_pred_np, columns=['minority', 'preds'])\n",
    "di_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5015462636947632"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 0.5s are predicted\n",
    "#input in Sigmoid function is too close to 0, thus, 0.5 returned. \n",
    "di_pred_df['preds'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning accuracy after disparate impact remover on bias dataset: 14.97%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    136055\n",
      "           1       0.15      1.00      0.26     23945\n",
      "\n",
      "    accuracy                           0.15    160000\n",
      "   macro avg       0.07      0.50      0.13    160000\n",
      "weighted avg       0.02      0.15      0.04    160000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0 136055]\n",
      " [     0  23945]]\n"
     ]
    }
   ],
   "source": [
    "#use model not trained disparate impact remover\n",
    "#predict using test transformed by disparate impact remover\n",
    "y_pred_dl_bias_di = FNN_rp.predict_classes(rp_test.features[:, 2:], verbose=0)\n",
    "\n",
    "accuracy_dl_bias_di = accuracy_score(y_pred_dl_bias_di, y_test)\n",
    "\n",
    "#print accuracy\n",
    "print('Deep Learning accuracy after disparate impact remover on bias dataset: ' + \n",
    "      str(round(accuracy_dl_bias_di*100,2)) + '%')\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_pred_dl_bias_di))\n",
    "\n",
    "print()\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_dl_bias_di))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparate Impact Remover - Down-sampled\n",
    "\n",
    "The results for disparate impact remover on bias dataset are disappointing. Therefore, we apply disparate impact remover to down-sampled dataset.\n",
    "\n",
    "On the down-sampled dataset, after applying disparate impact remover, only 0s (no default) are predicted for an accuracy of 85%. However, a model that only predicts 0s is not useful i.e. the model is not generalising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create disparate impact remover\n",
    "#preparation for next code cell below\n",
    "train_BLD_ds = BinaryLabelDataset(favorable_label='1',\n",
    "                                unfavorable_label='0',\n",
    "                                df=df_downsampled,\n",
    "                                label_names=['default'],\n",
    "                                protected_attribute_names=['minority'],\n",
    "                                unprivileged_protected_attributes=['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit disaparate impact remover on train\n",
    "di = DisparateImpactRemover(repair_level=1.0)\n",
    "rp_train_ds = di.fit_transform(train_BLD_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 160000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 8448.7256 - accuracy: 0.4880 - accuracy_1: 0.4880 - val_loss: 5989.8347 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 2/100\n",
      " - 0s - loss: 7789.2539 - accuracy: 0.4980 - accuracy_1: 0.4980 - val_loss: 5830.0907 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 3/100\n",
      " - 0s - loss: 7395.8545 - accuracy: 0.5180 - accuracy_1: 0.5180 - val_loss: 5601.0810 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 4/100\n",
      " - 0s - loss: 7989.7739 - accuracy: 0.4940 - accuracy_1: 0.4940 - val_loss: 5301.1136 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 5/100\n",
      " - 0s - loss: 7504.8662 - accuracy: 0.4990 - accuracy_1: 0.4990 - val_loss: 5002.4826 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 6/100\n",
      " - 0s - loss: 6991.9814 - accuracy: 0.5180 - accuracy_1: 0.5180 - val_loss: 4711.8496 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 7/100\n",
      " - 0s - loss: 6884.0806 - accuracy: 0.5300 - accuracy_1: 0.5300 - val_loss: 4374.6454 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 8/100\n",
      " - 0s - loss: 6857.8027 - accuracy: 0.4940 - accuracy_1: 0.4940 - val_loss: 4005.3560 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 9/100\n",
      " - 0s - loss: 7562.5679 - accuracy: 0.4650 - accuracy_1: 0.4650 - val_loss: 3590.9103 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 10/100\n",
      " - 0s - loss: 7237.9819 - accuracy: 0.5030 - accuracy_1: 0.5030 - val_loss: 3263.9976 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 11/100\n",
      " - 0s - loss: 5804.7319 - accuracy: 0.5240 - accuracy_1: 0.5240 - val_loss: 2946.6173 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 12/100\n",
      " - 0s - loss: 6493.1904 - accuracy: 0.5170 - accuracy_1: 0.5170 - val_loss: 2593.2433 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 13/100\n",
      " - 0s - loss: 5890.0596 - accuracy: 0.4770 - accuracy_1: 0.4770 - val_loss: 2286.0546 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 14/100\n",
      " - 0s - loss: 5821.1904 - accuracy: 0.5130 - accuracy_1: 0.5130 - val_loss: 1904.3785 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 15/100\n",
      " - 0s - loss: 6008.5918 - accuracy: 0.4980 - accuracy_1: 0.4980 - val_loss: 1535.9945 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 16/100\n",
      " - 0s - loss: 5536.0850 - accuracy: 0.5110 - accuracy_1: 0.5110 - val_loss: 1225.7846 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 17/100\n",
      " - 0s - loss: 5629.9321 - accuracy: 0.5130 - accuracy_1: 0.5130 - val_loss: 990.6911 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 18/100\n",
      " - 0s - loss: 4912.9399 - accuracy: 0.5090 - accuracy_1: 0.5090 - val_loss: 773.1183 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 19/100\n",
      " - 0s - loss: 4927.0698 - accuracy: 0.5090 - accuracy_1: 0.5090 - val_loss: 539.6386 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 20/100\n",
      " - 0s - loss: 4515.4463 - accuracy: 0.4950 - accuracy_1: 0.4950 - val_loss: 305.6059 - val_accuracy: 0.1497 - val_accuracy_1: 0.1497\n",
      "Epoch 21/100\n",
      " - 0s - loss: 4199.4321 - accuracy: 0.5120 - accuracy_1: 0.5120 - val_loss: 112.5210 - val_accuracy: 0.3903 - val_accuracy_1: 0.3903\n",
      "Epoch 22/100\n",
      " - 0s - loss: 4845.2983 - accuracy: 0.5020 - accuracy_1: 0.5020 - val_loss: 42.2651 - val_accuracy: 0.8461 - val_accuracy_1: 0.8461\n",
      "Epoch 23/100\n",
      " - 0s - loss: 4549.0029 - accuracy: 0.5020 - accuracy_1: 0.5020 - val_loss: 124.7977 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 24/100\n",
      " - 0s - loss: 4363.3481 - accuracy: 0.5030 - accuracy_1: 0.5030 - val_loss: 205.0175 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 25/100\n",
      " - 0s - loss: 4017.7405 - accuracy: 0.5180 - accuracy_1: 0.5180 - val_loss: 283.3640 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 26/100\n",
      " - 0s - loss: 3866.7078 - accuracy: 0.5130 - accuracy_1: 0.5130 - val_loss: 340.7044 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 27/100\n",
      " - 0s - loss: 3899.3250 - accuracy: 0.5380 - accuracy_1: 0.5380 - val_loss: 347.0874 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 28/100\n",
      " - 0s - loss: 3747.5767 - accuracy: 0.5120 - accuracy_1: 0.5120 - val_loss: 332.9023 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 29/100\n",
      " - 0s - loss: 3286.2820 - accuracy: 0.5060 - accuracy_1: 0.5060 - val_loss: 314.0652 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 30/100\n",
      " - 0s - loss: 3319.0452 - accuracy: 0.4810 - accuracy_1: 0.4810 - val_loss: 297.3369 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 31/100\n",
      " - 0s - loss: 2429.4624 - accuracy: 0.5510 - accuracy_1: 0.5510 - val_loss: 284.7884 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 32/100\n",
      " - 0s - loss: 3118.0649 - accuracy: 0.4970 - accuracy_1: 0.4970 - val_loss: 273.1537 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 33/100\n",
      " - 0s - loss: 2919.6575 - accuracy: 0.5350 - accuracy_1: 0.5350 - val_loss: 262.4139 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 34/100\n",
      " - 0s - loss: 2644.8528 - accuracy: 0.5110 - accuracy_1: 0.5110 - val_loss: 248.6191 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 35/100\n",
      " - 0s - loss: 2935.0503 - accuracy: 0.5070 - accuracy_1: 0.5070 - val_loss: 230.7465 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n",
      "Epoch 36/100\n",
      " - 0s - loss: 2853.9446 - accuracy: 0.5060 - accuracy_1: 0.5060 - val_loss: 212.7381 - val_accuracy: 0.8503 - val_accuracy_1: 0.8503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13cfd0f60>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downsampled dataset with disparate impact remover\n",
    "FNN_rp_ds = create_model(rp_train.features[:, 2:]) #first two columns dropped since sex and minority\n",
    "FNN_rp_ds.fit(rp_train_ds.features[:, 2:], rp_train_ds.labels, epochs=100, batch_size=1000, \n",
    "           shuffle=True, verbose=2, callbacks=[es_callback], validation_data=(rp_test.features[:, 2:], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minority</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   minority  preds\n",
       "0       1.0    0.0\n",
       "1       0.0    0.0\n",
       "2       0.0    0.0\n",
       "3       1.0    0.0\n",
       "4       1.0    0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict default rate\n",
    "di_preds_ds = FNN_rp_ds.predict(rp_test.features[:, 2:])\n",
    "\n",
    "#concat predictions with minority\n",
    "di_pred_np_ds = np.concatenate((test[['minority']],\n",
    "                                di_preds_ds), \n",
    "                                axis=1)\n",
    "\n",
    "#convert to dataframe\n",
    "di_pred_df_ds = pd.DataFrame(di_pred_np_ds, columns=['minority', 'preds'])\n",
    "di_pred_df_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 1s are predicted\n",
    "di_pred_df_ds['preds'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning accuracy after disparate impact remover on downsampled dataset: 85.03%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92    136055\n",
      "           1       0.00      0.00      0.00     23945\n",
      "\n",
      "    accuracy                           0.85    160000\n",
      "   macro avg       0.43      0.50      0.46    160000\n",
      "weighted avg       0.72      0.85      0.78    160000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136055      0]\n",
      " [ 23945      0]]\n"
     ]
    }
   ],
   "source": [
    "#use model not trained disparate impact remover\n",
    "#predict using test transformed by disparate impact remover\n",
    "y_pred_dl_ds_di = FNN_rp_ds.predict_classes(rp_test.features[:, 2:], verbose=0)\n",
    "\n",
    "accuracy_dl_ds_di = accuracy_score(y_pred_dl_ds_di, y_test)\n",
    "\n",
    "#print accuracy\n",
    "print('Deep Learning accuracy after disparate impact remover on downsampled dataset: ' + \n",
    "      str(round(accuracy_dl_ds_di*100,2)) + '%')\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_pred_dl_ds_di))\n",
    "\n",
    "print()\n",
    "print(confusion_matrix(y_test, y_pred_dl_ds_di))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
